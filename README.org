:PROPERTIES:
:CATEGORY: readme
:END:
#+STARTUP: overview indent
#+OPTIONS: toc:2

* Installation

The repo allows you train an active inference agent in a discrete, grid-world, custom environment, created using Gymnasium (https://gymnasium.farama.org/), a regularly maintained fork of Open AI's Gym library (which is no longer maintained).

This guide assumes that the user has installed [[https://git-scm.com/downloads][Git]] and Python (through [[https://www.anaconda.com/download][Anaconda]] or similar distributions) into their system.

1. Open a terminal and move into your preferred local folder or working directory:

   ~cd /home/working-dir~

2. Clone the Github repository (or download it into the same folder):

   ~git clone https://github.com/FilConscious/AiFGym.git~

3. Create Python virtual environment or conda environment with a recent version of Python, e.g.:

   ~conda create --name aifgym-env python=3.10~

4. Activate the environment:

   ~conda activate aifgym-env~

5. Install the package:

   ~pip install --editable .~

* Overview of the Repository

The core script is =../scripts/main.py= which defines an argument parser and passes the arguments provided through the command line to the train function of a task (Python) module defined in =../active_inf/tasks/= to train an active inference agent; the task module (e.g., =../active_inf/tasks/task1.py=) is imported dynamically depending on the task name the user has provided through the command line (see [[How to Run an Experiment]]).

Importantly, =../active_inf/= is the Python package (a folder with an =__init__.py= file) storing all the sub-packages and modules used to train an active inference agent. In addition to the task sub-package, it includes the following sub-packages:

- =../agents/= with the module =aif_agent.py= specifying the active inference agent class
- =../phts/= for defining the "phenotype" of an active inference agent in a certain environment, chiefly this involves specifying the desired observations/states of the agent (i.e., its priors) as well certain defining feature of the environment
- =../visuals/= contains modules for visualizing/plotting data from an experiment

In the working directory, there is also the package =../envs/= for defining (or downloading) different environments in which to train the agent, e.g., =../envs/grid_envs= is a sub-package storing modules for grid-world environments.

For example, =../active_inf/tasks/task1.py= is the Python module to train an active inference agent in a simple grid-world environment. The module imports the grid-world environment class from =../envs/grid_envs/grid_world_v0.py=, the active inference agent class from =../agents/aif_agent.py=, and its correspoding phenotype from =../phts/= (using an utility function in =../tasks/utils.py=). Then, it defines the train function instantiating at every run both an agent and the corresponding environment who interact for a certain number of episodes (training loop). The train function is called in =../scripts/main.py=, if ‘task1’ is the task name passed through the command line (so every new task module should have a train function).

* How to Run an Experiment

To train a vanilla active inference agent in a grid-like environment, you have to execute the main script from the terminal while passing to it the appropriate parameters.

More explicitly, after having cloned the repo (see [[Installation]]), you would execute the following instructions in the terminal (replace ~name-of-repo~ and ~name-of-env~ with the expression you pick to save the repo locally and the one for the conda environment you created, respectively):

1. Move into the local repo directory

   ~cd home/././name-of-repo/~

2. Activate conda environment

   ~conda activate name-of-env~

3. Execute Python script for training

   ~python -m scripts.main -task task1 -env GridWorldv0 -nr 1 -ne 2 -pt states -as kd~

4. Execute Python script for data visualization

   ~python -m active_inf.visuals.visualiz -i 4 -v 8 -ti 0 -tv 8 -vl 3 -hl 3~

What follows is a table summarizing the various arguments that could be used for instruction line (3); it should give you an idea of the kinds of experiments that can be run at the moment (or that potentially could be run, after some modifications/addition to the code).

| Argument             | Shorthand | Explanation                                            | Example                            |
|----------------------+-----------+--------------------------------------------------------+------------------------------------|
| ~--task_name~        | ~-task~   | Task identifier (involving a specific environment)     | ~task1~                            |
| ~--env_name~         | ~-evn~    | Environment on which the agent is trained              | ~GridWorldv0~                      |
| ~--num_runs~         | ~-nr~     | Number experiment runs                                 | ~30~ (default)                     |
| ~--num_episodes~     | ~-ne~     | Number of episodes for each run                        | ~100~ (default)                    |
| ~--pref_type~        | ~-pt~     | Whether the agent has preferred states or observations | ~states~ (default)                 |
| ~--action_selection~ | ~-as~     | Action selection strategy                              | ~kd~ (default)                     |
| ~--learn_A~          | ~-lA~     | Whether state-observation learning is enabled          | ~-lA~ (the value ~True~ is stored) |
| ~--learn_B~          | ~-lB~     | Whether state-transition learning is enabled           | ditto                              |
| ~--learn_D~          | ~-lD~     | Whether initial state learning is enabled              | ditto                              |
| ~--num_videos~       | ~-nvs~    | Number of recorded videos                              | ~10~                               |

What follows is a table summarizing the various arguments that could be used for instruction line (4):

| Argument          | Shorthand | Explanation                                                                                           | Example                       |
|-------------------+-----------+-------------------------------------------------------------------------------------------------------+-------------------------------|
| ~--step_fe_pi~    | ~-fpi~    | Timestep for which to plot the free energy                                                            | ~-1~ (the last step, default) |
| ~--x_ticks_estep~ | ~-xtes~   | Step for x-axis ticks (in plotting a variable as a function of episodes' number)                      | ~1~ (default)                 |
| ~--x_ticks_tstep~ | ~-xtts~   | Step for x-axis ticks (in plotting a variable as a function of total number of timesteps)             | ~50~ (default)                |
| ~--index_Si~      | ~-i~      | Index \(i\) for selecting a random variable \(S_{i}\)                                                    | ~0~ (default)                 |
| ~--value_Si~      | ~-v~      | Index \(j\) for selecting a value of \(S_{i}\) (used to plot \(Q(S_{i}= s_{j}\vert \pi)\) at a certain episode step) | ~0~                           |
| ~--index_tSi~     | ~-ti~     | Index \(i\) for selecting a random variable \(S_{i}\)                                                    | ~0~ (default)                 |
| ~--value_tSi~     | ~-tv~     | Index \(j\) for selecting a value of \(S_{i}\) (used to plot \(Q(S_{i}= s_{j}\vert \pi)\) at /every/ time step)      | ~0~                           |
| ~--state_A~       | ~-sa~     | Index \(i\) for selecting a \(Q(O_{i} = o_{j}\vert s_{})\) (a column of matrix \(\mathbf{A}\)) to plot             | ~0~ (default)                 |
| ~--state_B~       | ~-sb~     | Index \(i\) for selecting a \(Q_{a}(S_{j}_{}\vert S_{i})\) (a column of matrix \(\mathbf{B}\)) to plot                | ~0~ (default)                 |
| ~--action_B~      | ~-ab~     | Index \(a\) to pick the corresponding matrix \(\mathbf{B}\) to plot \(Q_{a}(S_{j}_{}\vert S_{i})\)                    | ~0~ (default)                 |
| ~--v_len~         | ~-vl~     | Height of the environment                                                                             | ~3~                           |
| ~--h_len~         | ~-hl~     | Width of the environment                                                                              | ~3~                           |


For a more detailed tutorial on the kinds of experiments one could run, see the companion paper.
* Resources

** Managing Python virtual enviroments

venv, conda, poetry

(more info on managing Python environments can be found in the Conda's [[https://docs.conda.io/projects/conda/en/stable/user-guide/index.html][User Guide]])

* References
