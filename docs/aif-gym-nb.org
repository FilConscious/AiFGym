:PROPERTIES:
:CATEGORY: notebook
:ID:       3bb5f4d8-a88a-4d41-8001-62982ba06f78
:END:
#+STARTUP: overview indent

* General Info
* Logs

** 2022-09-14: First simulations

Observations:

- after 50 episodes the agent is not able to learn the correct state-observation mapping
- the only state for which there seems to be some learning is state 5, i.e., \(P(O=5|S=5) = 0.30\)ish (which is very low anyway)
- I think that has some bad consequences for all the other measures/visualization, among others:

  1. the final categorical distributions conditioned on each policy, \(Q(S_{i}|\pi)\), are not correct, for example \(Q(S_{3}|\pi_{3})\)---which is the categorical distribution of the optimal policy (\(\pi_{3}\)) for the state at the third step---puts higher probability mass on state 7 when it should be on state 5;

  2. despite that, the agent consistently picks the optimal policy after 4 episodes, in fact if you look at the other \(Q(S_{i}|\pi_{3})\) they are more or less right so those categorical distributions can be used during planning to select the actions of the optimal policy and the desired trajectory;

  3. point (2) might be indication that perceptual inference is working correctly whereas the Dirichlet update of matrix \(\mathbf{A}\) is not giving the expected results, either because:

     - there is a bug in the code, or
     - when all data from all policies is collated you get nasty updates (consider that when you update the matrix \(\mathbf{A}\) you are using the distribution over states averaged over all policies, but if you have mainly followed one policy those other distribution are likely to be wrong and affect the update badly)

- for some reason all the policies probabilities go to zero at the end of the episode after the fourth episode, not sure if this is a bug
- even so, the optimal policy and the close-to-optimal policy seem to compete for probability mass after the fourth episde (but I don't understand why the both drop to zero at the last step)
- also, the total free energy is zero, this might be another bug

** 2022-09-15: Fixing some issues

Observations:

- after plotting the expected free energy throughout the experiment, I can confirm a couple things:

  1. the expected free energy computation (G) seems to be correct, the optimal policy indeed has a lower EFE than the other one

  2. at the last time step of every episode both policy EFEs go to zero, that means that by looking only at that quantity and updating Q(pi) the two policies will be equiprobable

  3. but (2) is simply the result of a design choice since at the last time step there is no longer any planning to do so the EFE for each policy was set to zero

- even so, (3) should not imply that \(Q(\pi_{1}) = Q(\pi_{2}) = 0.5\) at the end of the episode because in updating the policies probabilities we are also considering the extent to which each policy is able to minimize free energy, i.e., \(Q(\pi_{1}) = \sigma(-\mathcal{F} - \mathcal{G})\) where \(\mathcal{F}\) is the current free energy for \(\pi_{1}\) (it could be described as the evidence for the policy coming from observed outcomes)

- so I now suspect that there might be a problem with \(\mathcal{F}\) that might explain the reason why all the policies probabilities go to zero (observation from previous day)

- indeed, I now notice that the \(\mathcal{F}\) for each policy increases up to a certain point and eventually goes to zero at the end of every episode, I doubt this is right: the "wrong" policy should not have any evidence for it if the optimal policy was taken so it should have high free energy at the end of the episode, conversely the free energy for the optimal policy should decrease as the episode unfolds

- Okay, I fixed the previous issue. The problem was that I messed up the step counts for the training loops, so basically the last agent steps was not carried out, one certain consequence of the bug was that the last free energy slot in the storing matrix/vector stayed at the initialized value of zero (!), it may also be that since the last agent.step was not carried out some parameters were not update, specifically the matrices \(\mathbf{A}\) and \(\mathbf{B}\) (!)

** 2022-09-16: Importance of runs' average

Ran an experiment for 20 episodes to see if there was any improvement. Here are some observations:

- the free energis seem right now, increasing for the wrong policy and decreasing for the optimal one

- despite that, note that for both policies there is an increase in free energy as the episode unfolds, this can be explained by the presense of not-so-perfect state-observation mapping (matrix \(\mathbf{A}\), see observation below), so while you accumulate evidence (observations) for which you are not sure about the free energy might increase

- indeed, 20 episodes (and at the current learning rate, which I think it is just 1) do not seem enough for learning the full state-observation mapping

- there is some learning for the first two states (0 and 1) but at third state already (state 2) the agent is still confused and thinks that \(P(O=2|S=2) = 0.5\)ish, \(P(O=2|S=4) = 0.4\)ish, \(P(O=2|S=0) = 0.3\)ish, of course that affects inference because the agent might infer of being in state 4 after observing 2

- regardless, the agent is again able to pick the optimal policy consistently after a few episodes, having the right transition probability is likely to make this possible/easier even if the state-observation mapping is not perfect

- but likely because of the wrong state observation mapping it does not believe that by following such a policy will lead it to the goal state, i.e., \(Q(S_{8} = 8|\pi_1) = 0.0789\)ish, this appears really strange but it may be due (again) to the misguided state-observation mapping

- what is really strange is that there is somme oscillation at the beginning when it comes to \(Q(S_{8} = 8|\pi_{1})\) with peaks of \(0.8\) but it drops drastically after a few episodes

Ran an experiment for longer, 100 episodes with two different perceptual inference schemes:

1. Gradient descent (the same used until now)

- the drop in \(Q(S_{8} = 8|\pi_{1})\) observed earlier actually happens cyclically, it seems there is a kind of oscillation whereby the probability recovers and then drop again after a while, this might be indication that something is wrong with perceptual inference

- running for more episodes does not seem to lead to better state-observation mappings (matrix A)

2. Setting gradient to zero (analytical solution)

- here something weird happens: the optimal policy all of a sudden produces high free energy so it becomes the "worse" policy to the advantage of the suboptimal policy

- the agent ends up following the suboptimal policy (consistently with the fact that probability over policies is determined by \(\mathcal{F}\) and \(\mathcal{G}\))

- by following the suboptimal policy the agent never gets to the goal state but it is nevertheless compelled to follow it because of the smaller \(\mathcal{F}\)

- if all of that was not weird enough, this time the \(Q(S_{i}|\pi_{0})\) are near perfect even if the state-observation mapping (matrix \(\mathbf{A}\)) is not that comforting

*Hypothesis 1*: there is a problem with perceptual inference, it might be that doing the state updated simultaneuously is not beneficial and the correct variational update should be preferred.

*Hypothesis 2*: there is a problem with action selection at the beginning. Why is the action of the optimal policy not picked at the beginning? Even if its probability is higher? Action selection should be deterministic in this scenario \dots

Ran an experiment with NO learning over matrices \(\mathbf{A}\) and \(\mathbf{B}\):

- here everything seems to work fine

- however note the interaction in the update of \(Q(S|\pi_{0})\) with the wrong evidence

Ran another experiment for just a bunch of episodes with learning of matrix \(\mathbf{A}\) to see what happens at the beginning of the experiment:

- again, a scrambled state-observation mapping notwithstanding, the expected free energy makes action selection opting for the optimal policy

- however, the free energy value for the optimal policy turns out to be alarming, depsite the fact that the agent is collecting evidence for the optimal policy its free energy increases, the opposite happens for the suboptimal policy, this is really puzzling

- of course, if you were to include the \(\mathcal{F}\) in the action selection procedure, as things stand the worse policy would be selected instead

- another puzzling things is that the \(Q(S|\pi_{0})\) are near perfect even if that policy is rarely picked!

Explanation of what might be happening (come up after running): at the beginning of the experiment the agent might start going down the right path by picking the action from the optimal policy, note that at this stage the free energies associated with both policy might be very similar, in the middle of the path it may also happen that the free energy for the suboptimal policy turns out to be slightly smaller (why this happens is still a mistery) bringing the agent to select an action from the suboptimal policy, in a perverse turns of events (somehow) this leads to an even smaller free energy for the suboptimal policy, a pernicious cascade ensues so that at the next episode the agent keep selecting actions from the suboptimal policy; unfortunately expected free energy does not help either because the free energy becomes too big quickly (for the optimal policy thereby penalizing it) or because state-observation mapping remain all scrambled. Now, a bunch of hypothesis:

- it could be that the analytic implementation is not correct (note: these issues are not present in the gradient implementation)
- it could be this is just a quirk for this run and training for more than one agent might reveal a different picture (indeed, this might be an example of a bad bootstrap)
- Indeed, it was a quirk about the run, I forgot that for these kinds of randomness-imbued experiments it is crucial to average over runs/agents!

# Test 1.1: Learning State Observation Mapping (matrix A) with Knowledge of State-Transitions (matrix B)
# Test 2: Learning State Observation Mapping (matrix A) with Knowledge of State-Transitions (matrix B)

** 2023-07-03: Updated status

This was put on hold for several months (due to writing for the dissertation). The code is there but it may need some polishing before making the repo public and proper documentation. Also, the idea was to integrate the Monte-Carlo tree search planning component for a basic comparison with a RL agent.

** 2023-07-11: Added docs folder

I added a docs folder storing a file for the documentation, that can be accessed from the README.org, and this notebook for observations, notes, etc. (similar to what done for the predictive coding repo). Next, I should proceed to fill in those file with useful and essential information.

** 2023-07-17: Writing the Repo Overview

I'm going through the repo to summarize how it works and eliminate redundant, old files.

List of files that can be deleted:

- ~../scripts/utils_run.py~ can be deleted as it belongs to a previous implementation
-

** 2023-07-27: Issue about policy-independent state probabilities

I found some duplicate code that computes policy-independent state probabilities, reflecting a doubt that I had about /when/ to perform that computation, after state-estimation (perception) or after policy updating (planning)?

I think it makes more sense to do it after policy updating (and that is probably why I had commented out the first instance of those lines of code). However, those updated probabilities are usually not used by the agent, except maybe in the implementation of [[cite:&Sales2019]] (I need to double-check this).

** 2023-10-04: Pip issue with setuptools installation

I stumbled upon the following error when trying to install the AiFGym package with ~pip install -e .~:

~ERROR: Could not find a version that satisfies the requirement python==3.10 (from versions: none)~
~ERROR: No matching distribution found for python==3.10~

The error seems to be about ~pip~ not finding the required version of Python (as specified in the =project.toml= file of the package).

But I am running the installation inside a new conda environment created with an appropriate version of Python. So I don't understand why ~pip~ is throwing this error.

After removing two lines from the =project.toml= that specified Python as a requirement, the installation worked and all the dependencies were installed.

It seems the installation procedure failed to recognize the version of Python installed in the activated conda environment.

** 2023-10-05: Pip issue resolved

The mistake/bug was listing python among the dependencies in the =pyproject.tom=, having =requires-python = ">=3.10"= under =[project]= heading is sufficient.

** 2023-10-30: Errors with relative import

Upon trying to run a second experiment with active inference, I've got a relative import error. This might be due to some modification I made to configure the setuptools functionality for the installation of the repo.

The line throwing the error was in the =main.py=, i.e.:

~task_module = importlib.import_module("..active_inf.tasks." + task_module_name)~

And the complaint was ~ImportError: attempted relative import beyond top-level package~. This happens when you run the main script with ~python -m scripts.main -task task1 -env GridWorldv0 -nr 1 -ne 2 -pt states -as kd~ /inside/ the cloned repo directory.

I think that using that instruction tells Python to consider =script= as the top-level package (instead of =AiFGym=). When you want to import the task module with the =importlib= functionality, using the two dots asks Python to go beyong that top-level package, which is not allowed, apparently; rather, Python does not have or store information about what there is beyond that point (see these *stack overflow* answers [[https://stackoverflow.com/questions/30669474/beyond-top-level-package-error-in-relative-import][beyond top level package error in relative import]] and other links therein).

Before I added the setuptools functionality the code was working fine. I suspect that adding that functionality or adding some =ini.py= or changing the package structure was responsible for the error, but I cannot pinpoint exactly what produced it.

Thinking more about it, it might have to do with the fact that this time I cloned and /installed/ the package with ~pip install --editable .~ this time. According to the explanation at [[https://www.reddit.com/r/learnpython/comments/ayx7za/how_does_pip_install_e_work_is_there_a_specific/?rdt=36064][How does `pip install -e` work?]], that instruction installs (copies) the package in the =site-package= folder of your Python installation. Now the question is whether running the main Python module with  ~python -m AiFGym.scripts.main ...~ is actually using the site-package version of the package.

Mmmh... Maybe not because when I used ~python -m scripts.main -task ...~ there was no package indication. However, that might still be important because, as it is clear in the solution below, I have to specify the package name now, i.e., =AiFGym= in the ~importlib~ command. Python knows about that package becaue I have installed it.

The implemented solution involves two main corrections:

1. To specify that ~active_inf~ is a submodule of the ~AiFGym~ package, by writing:

   ~task_module = importlib.import_module(".active_inf.tasks." + task_module_name, "AiFGym")~

2. Run/invoke the Python module ~script.main~ from the directory in which the package ~AiFGym~ is located and by adding the specification of that package, i.e., ~AiFGym.script.main~. This is essential otherwise Python complains that it cannot find any ~AiFGym~ module after you make correction (1).

One of the downside of this solution is that the downloaded package (i.e., the cloned repo) cannot be renamed.

** 2023-10-31: ModuleNotFound Error

The blocking issue today was that running ~python -m AiFGym...~ threw a =ModuleNotFound= error. That was unexpected because installing the package with ~pip install -e .~ should make the package visible to Python (in the =site-package= of the Python installation of the created conda environment).

After browsing for more knowledge on how Python looks for modules locally, it turns out that the basic error I made was having all the code inside a directory whose name (=/./cde-AifGym=) did not correspond to the name of the installed package (=AiFGym=). That directory was stored in the =site-package= folder of the package, but this tells Python to look into /that/ folder for a package name =AiFGym=, which was not there.

The solution is simply to create another folder called =AiFGym= inside =/./cde-AiFGym/=. Actually it is not that simple because I'm using a flat layout so I have to run ~pip install -e .~ inside the =/AiFGym= folder which then makes Python look inside that folder for the package ~AiFGym~, but the package is not inside itself!

After some reflections, it seems all was due to a bad repo structure. To do successfully all the things above, you need to have a /project root directory/ which contains the =pyproject.toml=, the license, the =README.md= etc. as well as the main package you want to install/use, i.e., ~aifgym~. All the relevant code should go in there so it was a mistake to take out the =env= and the =scripts= folders in this case.

Now I have to update all the dependencies and pointers to folders inside the various files...then try again the installation and running some experiments. Let's see if I got it right this time!

** 2023-11-01: Still ModuleNotFound error when trying to run the module from any directory

The code works if I call the module from inside the project root directory (in which the package is located), but not if I am in any other directory.

A quick search online reveals that this might be due to the specification of appropriate *entry points* with ~setuptools~ in the =pyproject.toml=.

Okay, configured the new entry points. Basically, I removed the folder =/scripts= and copied and pasted the ~main()~ function in the =__init__.py= file of the package. Now it should be possible to just type ~run-aifgym~ in the terminal, from any folder, to train the agent.

In configuring this, I also had to change the location of the version attribute. This resulted in /another/ ~ModuleNotFound~ error. The mistake this time was to tell Python in the =pyproject.toml= to look for modules inside =aifgym=, but doing this makes Python blind about the fact that =aifgym= is a module itself during the building time (maybe that specification was responsible for other issue as well?).

** 2024-02-20: Re-familiarizing with the repo

Okay, trying to remember the last changes I applied. These relate to how to run the main scripts so I should experiment a bit and report results here. The instructions in the ~README.md~ should probably be updated as well.

1. with the new modifications using  ~python -m scripts.main -task task1 etc.~ to run an experiment does not work anymore
2. in fact I wrote last time that typing just ~run-aifgym~ should suffice, this is correct but now I get another ~ModuleNotFound~ error that requires investigation:

   - the problem was that now to perform a relative import with ~importlib.import_module(".phts." + sub_mod)~ you need to add the name of the package, i.e., "aifgym" in this case, so the right command is ~importlib.import_module(".phts." + sub_mod, "aifgym")~

3. for the visualizations I think the old method is still in place, I need to figure this out and maybe update it

   - yes, I can get the visualizations by running ~python -m aifgym.visuals.visualiz -i 4 -v 8 -ti 0 -tv 8 -vl 3 -hl 3~ inside the project folder
   - the idea would be to have a command like ~vis-aifgym~ that can be used anywhere to get the pictures, this should require only adding a new line in the corresponding section of the toml, no clear where it should be pointing to though, the ~visualiz.py~ file is enough?
   - got it, I had a ~main()~ function for the visualizations as well, so by analogy I should simply move that into the ~__init__.py~, yeah, but which one? I think it should be the top-level one
   - got it, it actually works by simply adding a pointer to a sub-level package. i.e., ~visuals~, and calling the function ~main()~ now moved into the init file inside ~visuals~
   - so I guess we can delete ~visualiz.py~ now

** 2024-02-22: README and docs are up and running

A few issues remain:

1. need to add a few more details about resources, with various links to other relevant works
2. probably need to add a few more details about the implemented features and the experiments that can be run
3. need to sort out the math delimiters issue due to Github flavoured Markdown quirks

** 2024-02-23: Bug in the Logging of EFE data when learning matrix B

Found a potential bug in the scenario where matrix B is supposed to be learn. The bug has to do with the expected free energy components which appears to be all zero when the corresponding plotting functions loads the data.

This is strange because the expected free energy is not zero during the experiment (need to double check this) so it might be a problem with the logging or loading of the data.

** 2024-02-29: EFE data bug fixed

For the case of matrix B learning, I forgot to include the instructions to store the values of the different free energy components.

** 2024-03-01: Policy learning challenging when B matrix not available

Finally, the day came to run the second series of experiments. Things did not turn out as expected as the results were rather disappointing. I seem to remember that I had encounter something similar in the past, probably when experimenting with learning the B matrix, but I found no record of it.

The main issue is that no matter how many runs (agents) you simulate and average upon it appears that the active inference algorithm does not favour the optimal policy as there seems to be more agents going through the wrong sensorimotor trajectory using policy 0 (the one not leading to the goal state), instead of policy 1.

In general, the plots reflect this state of affairs, indicating that these results are not a complete fluke. At convergence, after a substantial number of episodes (e.g., 250), policy 0 minimizes free-energy to a greater degree than policy 1, which is consistent with the fact that the former policy has a higher probability than the latter.

The gap is not enormous, suggesting that overall the two trajectories might be perceived as similar or equal; this would indicate that the agent has not learned the consequences of its actions very well.

A suggestion in this direction is offered by the two heat maps showing the probabilities \(Q(S_{i}|\pi_{0}\) and \(Q(S_{i}|\pi_{1})\) for all the states. They are pretty much the same, with the crucial feature that each seems to spread the probabilities among the states that each policy would visit. In other words, it is like the algorithm tends (on average) to store the same sensorimotor information---from both policies---in the representation of each policy, with the consequence that the probabilities have to be equally spread (more or less).

Now, in writing this, I come to wonder if this particular and peculiar state of affairs is actually a by-product of the averaging: it might be that half of the agents manage to pick the right policy and the other half doesn't, resulting in this weird mesh of sensorimotor information. This might be related to the bad bootstrap flagged by [[cite:&Tschantz2020b]].

However, if this is indeed an artifact from averaging, one might wonder: does it make sense to average? The reason I did it is because I remmeber reading this suggestion in [[cite:&Sutton2018]] and is normally done in RL.

Another weird result can be seen in the plot of the free energy components, in one of the episode towards the end, the B-novelty for policy 1 skyrockets which leave one puzzled as to why then policy 1 is not preferred over policy 0. The problem is that this is likely an outlier that drastically skews the free energy component plot (again, this reports an average over the runs) and has no effect on the policies probabilities (it might be just a single agent).

Actionable steps from here:

1. check the transition matrices for each action to see if indeed there is widespread confusion on the most likely consequences of each action, this is what I would expect after looking at the previous plots (again, this might be an averaging artifact)
   
2. it could be interesting to add a data selection parameter to plot the data only for agents that manage to select the right policy and for those which don't at convergence, this would provide an indication of what the split might be like

3. an interesting experiment at this point would be to change the vector of prior preferences of the agents, to include a preference over intermediate states, and run another batch of simulation, hoping to see that:

   - with intermediate preferences the algorithm would be facilitated in converging to the right policy on average

   - that is, the majority of runs (agents) would be able to pick the right policy and learn distinct sensorimotor consequences of their actions

4. concurrently with (1) and (2) I would start writing down the salient findings in the corresponding section in the draft, roughly:

   - report the average results and expose the weirdness, I wouldn't spend too much on this (indeed, recycle the observations that you wrote above) if all turns out to be an artifact of averaging

   - I would raise the question about averaging, point to the bad bootstrap problem, and compare to standard RL

   - I would report the results from (3) hoping to make the clear contribution that the badbootstrap problem can be ameliorated with preferences over intermediate states; in other words, no sparse reward, another comparison with RL might be useful here

Observations from taking action on the above steps:

1. checking the transition for each action:

   - action 0 (“go up”): this is an action that no policy dictates and yet it seems it has been selected a few times so that the agent has learned correctly that from state 7 going up make you stay there, and the same for state 1, these are the only two states for which the probabilities have increased a bit but not to a point of near-certainty, reflecting correctly that the action has not been picked many times

   - action 1 ("go right"): this is an action that both policies dictate in different states in the maze, the transition probabilities for those states show a correct increase, being in the interval 0.4-0.6

   - action 2 (“go down”): again this is an action that both policies dictate in different states in the maze, the transition probabilities appear again to have increased appropriately (by a magnitude comparable to that of action 1), reflecting the fact that the average agent has learned the sensorimotor consequences of its actions on different paths

   - action 3 (“go left”): this is an action that neither policy dictates, as expected the heatmap shows that no learning has occurred so the transition probabilities have remained low for all the states.

   - Overall, the average active inference agent seems to have learned the correct consequences of its actions for both policies, this is evidence consistent with the fact that such an agent is actually a mesh-up of optimal and sub-optimal agents.

2. added the functionality to plot a subset of the runs depending on which policy is assigned higher probability, a few observations:

   - the plots now look much more consistent and confirm the speculations above

   - there is still a weird result: the skyrocketing of the B-novelty at some point during the experiment, more investigation is required to understand what is going on there

   - also, the false/wrong state beliefs for the sub-optimal policy are still present, I think this deserves further attention and an explanation in the paper, as briefly remarked above I think this is due to the way free energy minimization is achieved.
