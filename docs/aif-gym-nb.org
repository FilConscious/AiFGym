:PROPERTIES:
:CATEGORY: notebook
:ID:       3bb5f4d8-a88a-4d41-8001-62982ba06f78
:END:
#+STARTUP: overview indent

* General Info
* Logs

** [2022-09-14]: First simulations

Observations:

- after 50 episodes the agent is not able to learn the correct state-observation mapping
- the only state for which there seems to be some learning is state 5, i.e., \(P(O=5|S=5) = 0.30\)ish (which is very low anyway)
- I think that has some bad consequences for all the other measures/visualization, among others:

  1. the final categorical distributions conditioned on each policy, \(Q(S_{i}|\pi)\), are not correct, for example \(Q(S_{3}|\pi_{3})\)---which is the categorical distribution of the optimal policy (\(\pi_{3}\)) for the state at the third step---puts higher probability mass on state 7 when it should be on state 5;

  2. despite that, the agent consistently picks the optimal policy after 4 episodes, in fact if you look at the other \(Q(S_{i}|\pi_{3})\) they are more or less right so those categorical distributions can be used during planning to select the actions of the optimal policy and the desired trajectory;

  3. point (2) might be indication that perceptual inference is working correctly whereas the Dirichlet update of matrix \(\mathbf{A}\) is not giving the expected results, either because:

     - there is a bug in the code, or
     - when all data from all policies is collated you get nasty updates (consider that when you update the matrix \(\mathbf{A}\) you are using the distribution over states averaged over all policies, but if you have mainly followed one policy those other distribution are likely to be wrong and affect the update badly)

- for some reason all the policies probabilities go to zero at the end of the episode after the fourth episode, not sure if this is a bug
- even so, the optimal policy and the close-to-optimal policy seem to compete for probability mass after the fourth episde (but I don't understand why the both drop to zero at the last step)
- also, the total free energy is zero, this might be another bug

** [2022-09-15]: Fixing some issues

Observations:

- after plotting the expected free energy throughout the experiment, I can confirm a couple things:

  1. the expected free energy computation (G) seems to be correct, the optimal policy indeed has a lower EFE than the other one

  2. at the last time step of every episode both policy EFEs go to zero, that means that by looking only at that quantity and updating Q(pi) the two policies will be equiprobable

  3. but (2) is simply the result of a design choice since at the last time step there is no longer any planning to do so the EFE for each policy was set to zero

- even so, (3) should not imply that \(Q(\pi_{1}) = Q(\pi_{2}) = 0.5\) at the end of the episode because in updating the policies probabilities we are also considering the extent to which each policy is able to minimize free energy, i.e., \(Q(\pi_{1}) = \sigma(-\mathcal{F} - \mathcal{G})\) where \(\mathcal{F}\) is the current free energy for \(\pi_{1}\) (it could be described as the evidence for the policy coming from observed outcomes)

- so I now suspect that there might be a problem with \(\mathcal{F}\) that might explain the reason why all the policies probabilities go to zero (observation from previous day)

- indeed, I now notice that the \(\mathcal{F}\) for each policy increases up to a certain point and eventually goes to zero at the end of every episode, I doubt this is right: the "wrong" policy should not have any evidence for it if the optimal policy was taken so it should have high free energy at the end of the episode, conversely the free energy for the optimal policy should decrease as the episode unfolds

- Okay, I fixed the previous issue. The problem was that I messed up the step counts for the training loops, so basically the last agent steps was not carried out, one certain consequence of the bug was that the last free energy slot in the storing matrix/vector stayed at the initialized value of zero (!), it may also be that since the last agent.step was not carried out some parameters were not update, specifically the matrices \(\mathbf{A}\) and \(\mathbf{B}\) (!)

** [2022-09-16]: Importance of runs' average

Ran an experiment for 20 episodes to see if there was any improvement. Here are some observations:

- the free energis seem right now, increasing for the wrong policy and decreasing for the optimal one

- despite that, note that for both policies there is an increase in free energy as the episode unfolds, this can be explained by the presense of not-so-perfect state-observation mapping (matrix \(\mathbf{A}\), see observation below), so while you accumulate evidence (observations) for which you are not sure about the free energy might increase

- indeed, 20 episodes (and at the current learning rate, which I think it is just 1) do not seem enough for learning the full state-observation mapping

- there is some learning for the first two states (0 and 1) but at third state already (state 2) the agent is still confused and thinks that \(P(O=2|S=2) = 0.5\)ish, \(P(O=2|S=4) = 0.4\)ish, \(P(O=2|S=0) = 0.3\)ish, of course that affects inference because the agent might infer of being in state 4 after observing 2

- regardless, the agent is again able to pick the optimal policy consistently after a few episodes, having the right transition probability is likely to make this possible/easier even if the state-observation mapping is not perfect

- but likely because of the wrong state observation mapping it does not believe that by following such a policy will lead it to the goal state, i.e., \(Q(S_{8} = 8|\pi_1) = 0.0789\)ish, this appears really strange but it may be due (again) to the misguided state-observation mapping

- what is really strange is that there is somme oscillation at the beginning when it comes to \(Q(S_{8} = 8|\pi_{1})\) with peaks of \(0.8\) but it drops drastically after a few episodes

Ran an experiment for longer, 100 episodes with two different perceptual inference schemes:

1. Gradient descent (the same used until now)

- the drop in \(Q(S_{8} = 8|\pi_{1})\) observed earlier actually happens cyclically, it seems there is a kind of oscillation whereby the probability recovers and then drop again after a while, this might be indication that something is wrong with perceptual inference

- running for more episodes does not seem to lead to better state-observation mappings (matrix A)

2. Setting gradient to zero (analytical solution)

- here something weird happens: the optimal policy all of a sudden produces high free energy so it becomes the "worse" policy to the advantage of the suboptimal policy

- the agent ends up following the suboptimal policy (consistently with the fact that probability over policies is determined by \(\mathcal{F}\) and \(\mathcal{G}\))

- by following the suboptimal policy the agent never gets to the goal state but it is nevertheless compelled to follow it because of the smaller \(\mathcal{F}\)

- if all of that was not weird enough, this time the \(Q(S_{i}|\pi_{0})\) are near perfect even if the state-observation mapping (matrix \(\mathbf{A}\)) is not that comforting

*Hypothesis 1*: there is a problem with perceptual inference, it might be that doing the state updated simultaneuously is not beneficial and the correct variational update should be preferred.

*Hypothesis 2*: there is a problem with action selection at the beginning. Why is the action of the optimal policy not picked at the beginning? Even if its probability is higher? Action selection should be deterministic in this scenario \dots

Ran an experiment with NO learning over matrices \(\mathbf{A}\) and \(\mathbf{B}\):

- here everything seems to work fine

- however note the interaction in the update of \(Q(S|\pi_{0})\) with the wrong evidence

Ran another experiment for just a bunch of episodes with learning of matrix \(\mathbf{A}\) to see what happens at the beginning of the experiment:

- again, a scrambled state-observation mapping notwithstanding, the expected free energy makes action selection opting for the optimal policy

- however, the free energy value for the optimal policy turns out to be alarming, depsite the fact that the agent is collecting evidence for the optimal policy its free energy increases, the opposite happens for the suboptimal policy, this is really puzzling

- of course, if you were to include the \(\mathcal{F}\) in the action selection procedure, as things stand the worse policy would be selected instead

- another puzzling things is that the \(Q(S|\pi_{0})\) are near perfect even if that policy is rarely picked!

Explanation of what might be happening (come up after running): at the beginning of the experiment the agent might start going down the right path by picking the action from the optimal policy, note that at this stage the free energies associated with both policy might be very similar, in the middle of the path it may also happen that the free energy for the suboptimal policy turns out to be slightly smaller (why this happens is still a mistery) bringing the agent to select an action from the suboptimal policy, in a perverse turns of events (somehow) this leads to an even smaller free energy for the suboptimal policy, a pernicious cascade ensues so that at the next episode the agent keep selecting actions from the suboptimal policy; unfortunately expected free energy does not help either because the free energy becomes too big quickly (for the optimal policy thereby penalizing it) or because state-observation mapping remain all scrambled. Now, a bunch of hypothesis:

- it could be that the analytic implementation is not correct (note: these issues are not present in the gradient implementation)
- it could be this is just a quirk for this run and training for more than one agent might reveal a different picture (indeed, this might be an example of a bad bootstrap)
- Indeed, it was a quirk about the run, I forgot that for these kinds of randomness-imbued experiments it is crucial to average over runs/agents!

# Test 1.1: Learning State Observation Mapping (matrix A) with Knowledge of State-Transitions (matrix B)
# Test 2: Learning State Observation Mapping (matrix A) with Knowledge of State-Transitions (matrix B)

** [2023-07-03]: Updated status

This was put on hold for several months (due to writing for the dissertation). The code is there but it may need some polishing before making the repo public and proper documentation. Also, the idea was to integrate the Monte-Carlo tree search planning component for a basic comparison with a RL agent.

** [2023-07-11]: Added docs folder

I added a docs folder storing a file for the documentation, that can be accessed from the README.org, and this notebook for observations, notes, etc. (similar to what done for the predictive coding repo). Next, I should proceed to fill in those file with useful and essential information.

** [2023-07-17]: Writing the Repo Overview

I'm going through the repo to summarize how it works and eliminate redundant, old files.

List of files that can be deleted:

- ~../scripts/utils_run.py~ can be deleted as it belongs to a previous implementation
-

** [2023-07-27]: Issue about policy-independent state probabilities

I found some duplicate code that computes policy-independent state probabilities, reflecting a doubt that I had about /when/ to perform that computation, after state-estimation (perception) or after policy updating (planning)?

I think it makes more sense to do it after policy updating (and that is probably why I had commented out the first instance of those lines of code). However, those updated probabilities are usually not used by the agent, except maybe in the implementation of [[cite:&Sales2019]] (I need to double-check this).

** [2023-10-04]: Pip issue with setuptools installation

I stumbled upon the following error when trying to install the AiFGym package with ~pip install -e .~:

~ERROR: Could not find a version that satisfies the requirement python==3.10 (from versions: none)~
~ERROR: No matching distribution found for python==3.10~

The error seems to be about ~pip~ not finding the required version of Python (as specified in the =project.toml= file of the package).

But I am running the installation inside a new conda environment created with an appropriate version of Python. So I don't understand why ~pip~ is throwing this error.

After removing two lines from the =project.toml= that specified Python as a requirement, the installation worked and all the dependencies were installed.

It seems the installation procedure failed to recognize the version of Python installed in the activated conda environment.

** [2023-10-05]: Pip issue resolved

The mistake/bug was listing python among the dependencies in the =pyproject.tom=, having =requires-python = ">=3.10"= under =[project]= heading is sufficient.

** [2023-10-30]: Errors with relative import

Upon trying to run a second experiment with active inference, I've got a relative import error. This might be due to some modification I made to configure the setuptools functionality for the installation of the repo.

The line throwing the error was in the =main.py=, i.e.:

~task_module = importlib.import_module("..active_inf.tasks." + task_module_name)~

And the complaint was ~ImportError: attempted relative import beyond top-level package~. This happens when you run the main script with ~python -m scripts.main -task task1 -env GridWorldv0 -nr 1 -ne 2 -pt states -as kd~ /inside/ the cloned repo directory.

I think that using that instruction tells Python to consider =script= as the top-level package (instead of =AiFGym=). When you want to import the task module with the =importlib= functionality, using the two dots asks Python to go beyong that top-level package, which is not allowed, apparently; rather, Python does not have or store information about what there is beyond that point (see these *stack overflow* answers [[https://stackoverflow.com/questions/30669474/beyond-top-level-package-error-in-relative-import][beyond top level package error in relative import]] and other links therein).

Before I added the setuptools functionality the code was working fine. I suspect that adding that functionality or adding some =ini.py= or changing the package structure was responsible for the error, but I cannot pinpoint exactly what produced it.

Thinking more about it, it might have to do with the fact that this time I cloned and /installed/ the package with ~pip install --editable .~ this time. According to the explanation at [[https://www.reddit.com/r/learnpython/comments/ayx7za/how_does_pip_install_e_work_is_there_a_specific/?rdt=36064][How does `pip install -e` work?]], that instruction installs (copies) the package in the =site-package= folder of your Python installation. Now the question is whether running the main Python module with  ~python -m AiFGym.scripts.main ...~ is actually using the site-package version of the package.

Mmmh... Maybe not because when I used ~python -m scripts.main -task ...~ there was no package indication. However, that might still be important because, as it is clear in the solution below, I have to specify the package name now, i.e., =AiFGym= in the ~importlib~ command. Python knows about that package becaue I have installed it.

The implemented solution involves two main corrections:

1. To specify that ~active_inf~ is a submodule of the ~AiFGym~ package, by writing:

   ~task_module = importlib.import_module(".active_inf.tasks." + task_module_name, "AiFGym")~

2. Run/invoke the Python module ~script.main~ from the directory in which the package ~AiFGym~ is located and by adding the specification of that package, i.e., ~AiFGym.script.main~. This is essential otherwise Python complains that it cannot find any ~AiFGym~ module after you make correction (1).

One of the downside of this solution is that the downloaded package (i.e., the cloned repo) cannot be renamed.

** [2023-10-31]: ModuleNotFound Error

The blocking issue today was that running ~python -m AiFGym...~ threw a =ModuleNotFound= error. That was unexpected because installing the package with ~pip install -e .~ should make the package visible to Python (in the =site-package= of the Python installation of the created conda environment).

After browsing for more knowledge on how Python looks for modules locally, it turns out that the basic error I made was having all the code inside a directory whose name (=/./cde-AifGym=) did not correspond to the name of the installed package (=AiFGym=). That directory was stored in the =site-package= folder of the package, but this tells Python to look into /that/ folder for a package name =AiFGym=, which was not there.

The solution is simply to create another folder called =AiFGym= inside =/./cde-AiFGym/=. Actually it is not that simple because I'm using a flat layout so I have to run ~pip install -e .~ inside the =/AiFGym= folder which then makes Python look inside that folder for the package ~AiFGym~, but the package is not inside itself!

After some reflections, it seems all was due to a bad repo structure. To do successfully all the things above, you need to have a /project root directory/ which contains the =pyproject.toml=, the license, the =README.md= etc. as well as the main package you want to install/use, i.e., ~aifgym~. All the relevant code should go in there so it was a mistake to take out the =env= and the =scripts= folders in this case.

Now I have to update all the dependencies and pointers to folders inside the various files...then try again the installation and running some experiments. Let's see if I got it right this time!

** [2023-11-01]: Still ModuleNotFound error when trying to run the module from any directory

The code works if I call the module from inside the project root directory (in which the package is located), but not if I am in any other directory.

A quick search online reveals that this might be due to the specification of appropriate *entry points* with ~setuptools~ in the =pyproject.toml=.

Okay, configured the new entry points. Basically, I removed the folder =/scripts= and copied and pasted the ~main()~ function in the =__init__.py= file of the package. Now it should be possible to just type ~run-aifgym~ in the terminal, from any folder, to train the agent.

In configuring this, I also had to change the location of the version attribute. This resulted in /another/ ~ModuleNotFound~ error. The mistake this time was to tell Python in the =pyproject.toml= to look for modules inside =aifgym=, but doing this makes Python blind about the fact that =aifgym= is a module itself during the building time (maybe that specification was responsible for other issue as well?).

** [2024-02-20]: Re-familiarizing with the repo

Okay, trying to remember the last changes I applied. These relate to how to run the main scripts so I should experiment a bit and report results here. The instructions in the ~README.md~ should probably be updated as well.

1. with the new modifications using  ~python -m scripts.main -task task1 etc.~ to run an experiment does not work anymore
2. in fact I wrote last time that typing just ~run-aifgym~ should suffice, this is correct but now I get another ~ModuleNotFound~ error that requires investigation:

   - the problem was that now to perform a relative import with ~importlib.import_module(".phts." + sub_mod)~ you need to add the name of the package, i.e., "aifgym" in this case, so the right command is ~importlib.import_module(".phts." + sub_mod, "aifgym")~

3. for the visualizations I think the old method is still in place, I need to figure this out and maybe update it

   - yes, I can get the visualizations by running ~python -m aifgym.visuals.visualiz -i 4 -v 8 -ti 0 -tv 8 -vl 3 -hl 3~ inside the project folder
   - the idea would be to have a command like ~vis-aifgym~ that can be used anywhere to get the pictures, this should require only adding a new line in the corresponding section of the toml, no clear where it should be pointing to though, the ~visualiz.py~ file is enough?
   - got it, I had a ~main()~ function for the visualizations as well, so by analogy I should simply move that into the ~__init__.py~, yeah, but which one? I think it should be the top-level one
   - got it, it actually works by simply adding a pointer to a sub-level package. i.e., ~visuals~, and calling the function ~main()~ now moved into the init file inside ~visuals~
   - so I guess we can delete ~visualiz.py~ now

** [2024-02-22]: README and docs are up and running

A few issues remain:

1. need to add a few more details about resources, with various links to other relevant works
2. probably need to add a few more details about the implemented features and the experiments that can be run
3. need to sort out the math delimiters issue due to Github flavoured Markdown quirks

** [2024-02-23]: Bug in the Logging of EFE data when learning matrix B

Found a potential bug in the scenario where matrix B is supposed to be learn. The bug has to do with the expected free energy components which appears to be all zero when the corresponding plotting functions loads the data.

This is strange because the expected free energy is not zero during the experiment (need to double check this) so it might be a problem with the logging or loading of the data.

** [2024-02-29]: EFE data bug fixed

For the case of matrix B learning, I forgot to include the instructions to store the values of the different free energy components.

** [2024-03-01]: Policy learning challenging when B matrix not available

Finally, the day came to run the second series of experiments. Things did not turn out as expected as the results were rather disappointing. I seem to remember that I had encounter something similar in the past, probably when experimenting with learning the B matrix, but I found no record of it.

The main issue is that no matter how many runs (agents) you simulate and average upon it appears that the active inference algorithm does not favour the optimal policy as there seems to be more agents going through the wrong sensorimotor trajectory using policy 0 (the one not leading to the goal state), instead of policy 1.

In general, the plots reflect this state of affairs, indicating that these results are not a complete fluke. At convergence, after a substantial number of episodes (e.g., 250), policy 0 minimizes free-energy to a greater degree than policy 1, which is consistent with the fact that the former policy has a higher probability than the latter.

The gap is not enormous, suggesting that overall the two trajectories might be perceived as similar or equal; this would indicate that the agent has not learned the consequences of its actions very well.

A suggestion in this direction is offered by the two heat maps showing the probabilities \(Q(S_{i}|\pi_{0}\) and \(Q(S_{i}|\pi_{1})\) for all the states. They are pretty much the same, with the crucial feature that each seems to spread the probabilities among the states that each policy would visit. In other words, it is like the algorithm tends (on average) to store the same sensorimotor information---from both policies---in the representation of each policy, with the consequence that the probabilities have to be equally spread (more or less).

Now, in writing this, I come to wonder if this particular and peculiar state of affairs is actually a by-product of the averaging: it might be that half of the agents manage to pick the right policy and the other half doesn't, resulting in this weird mesh of sensorimotor information. This might be related to the bad bootstrap flagged by [[cite:&Tschantz2020b]].

However, if this is indeed an artifact from averaging, one might wonder: does it make sense to average? The reason I did it is because I remmeber reading this suggestion in [[cite:&Sutton2018]] and is normally done in RL.

Another weird result can be seen in the plot of the free energy components, in one of the episode towards the end, the B-novelty for policy 1 skyrockets which leave one puzzled as to why then policy 1 is not preferred over policy 0. The problem is that this is likely an outlier that drastically skews the free energy component plot (again, this reports an average over the runs) and has no effect on the policies probabilities (it might be just a single agent).

Actionable steps from here:

1. check the transition matrices for each action to see if indeed there is widespread confusion on the most likely consequences of each action, this is what I would expect after looking at the previous plots (again, this might be an averaging artifact)
   
2. it could be interesting to add a data selection parameter to plot the data only for agents that manage to select the right policy and for those which don't at convergence, this would provide an indication of what the split might be like

3. an interesting experiment at this point would be to change the vector of prior preferences of the agents, to include a preference over intermediate states, and run another batch of simulation, hoping to see that:

   - with intermediate preferences the algorithm would be facilitated in converging to the right policy on average

   - that is, the majority of runs (agents) would be able to pick the right policy and learn distinct sensorimotor consequences of their actions

4. concurrently with (1) and (2) I would start writing down the salient findings in the corresponding section in the draft, roughly:

   - report the average results and expose the weirdness, I wouldn't spend too much on this (indeed, recycle the observations that you wrote above) if all turns out to be an artifact of averaging

   - I would raise the question about averaging, point to the bad bootstrap problem, and compare to standard RL

   - I would report the results from (3) hoping to make the clear contribution that the badbootstrap problem can be ameliorated with preferences over intermediate states; in other words, no sparse reward, another comparison with RL might be useful here

Observations from taking action on the above steps:

1. checking the transition for each action:

   - action 0 (“go up”): this is an action that no policy dictates and yet it seems it has been selected a few times so that the agent has learned correctly that from state 7 going up make you stay there, and the same for state 1, these are the only two states for which the probabilities have increased a bit but not to a point of near-certainty, reflecting correctly that the action has not been picked many times

   - action 1 ("go right"): this is an action that both policies dictate in different states in the maze, the transition probabilities for those states show a correct increase, being in the interval 0.4-0.6

   - action 2 (“go down”): again this is an action that both policies dictate in different states in the maze, the transition probabilities appear again to have increased appropriately (by a magnitude comparable to that of action 1), reflecting the fact that the average agent has learned the sensorimotor consequences of its actions on different paths

   - action 3 (“go left”): this is an action that neither policy dictates, as expected the heatmap shows that no learning has occurred so the transition probabilities have remained low for all the states.

   - Overall, the average active inference agent seems to have learned the correct consequences of its actions for both policies, this is evidence consistent with the fact that such an agent is actually a mesh-up of optimal and sub-optimal agents.

2. added the functionality to plot a subset of the runs depending on which policy is assigned higher probability, a few observations:

   - the plots now look much more consistent and confirm the speculations above

   - there is still a weird result: the skyrocketing of the B-novelty at some point during the experiment, more investigation is required to understand what is going on there

   - also, the false/wrong state beliefs for the sub-optimal policy are still present, I think this deserves further attention and an explanation in the paper, as briefly remarked above I think this is due to the way free energy minimization is achieved.

** [2024-03-19]: Policy learning without B matrices

The issue was that there are optimal and suboptimal agents so when you look at the average results of all the runs you get the impression that on average active inference agents do not find the optimal policy, they are maladaptive.

I added some code to separate good and bad runs and get better visualizations of the learning behaviour for both kinds of agents. I was also reminded of the bad-bootstrap problem, as potential explanation for these failures of active inference, but after re-looking at the paper I realized that AT meant something else.

After some reflections, and staring at the formulas for free energy minimization for a while, I realized that the problem might be related to the quantity being used in the current implementation. This was taken from the tutorial paper by Da Costa and does not have the policy probability as a weighting term of the free energy expression. This might explain why for both agents, the state predictions for both policy are nearly identical: the observations are used to update the policy-conditioned beliefs regardless of how probable the policy itself is.

I need to look at the code again and implement the correct objective, and hope that something changes.

** [2024-03-20]: No changes

Things seem harder than what I thought. Taking into consideration the probabilities over policies in the objective does not seem to work: 1) it does not solve what I thought it could solve, 2) it makes things worse because now the agent does not seem to be learning as well as before.

I decided to start writing the section of the draft regardless, with what I have so far. Unfortunately, I stumbled onto another issue. When I plot the runs selectively, depending on final policy probabilities, there is a weird result in the EFE plot: basically the risk is high for the policy that has the highest probability, which does not makes sense.

Further investigations revealed that the EFE computation is correct at the second-last step before reaching the final state, but it is not before that. It looks as if those computations accumulare more errors for the correct policy than the wrong one.

** [2024-03-21]: Tracked down the reason for weird EFE computations and other things

After hours spent examining the printed info in the terminal during various simulations, I was able to pinpoint the exact reasons for why the weird behaviours described earlier are happening.

The weird results:

1. the agent gets stuck into performing either \(\pi_{0}\) or \(\pi_{1}\), with no ability to switch policy perhaps thanks to the risk or exploratory bonuses in the expected free energy
2. moreover, while the agent can learn the state transitions for the chosen policy correctly, the expected free energy reveals something unexpected: the risk turns out to be higher for the chosen policy!
3. that means, say, that if the agent has followed \(\pi_{0}\) its risk is higher, similar for \(\pi_{1}\) (without any regard for whether the policy leads to the goal state or not)
4. two observations are pressing at this point:

   1. why is the risk behaving in such a way?
   2. why, despite this high risk, is the agent unable to switch to the other policy?

5. the answer to these questions hinges in part upon the vector of state preferences used in the computation of the risk:

   - the vector used attributes equal probability to all the state values except for the last time step where the agent wants to be at the goal state

   - in general, this creates a problem the moment the agent pursues a policy and gathers evidence about the state probabilities along a certain sensorimotor path

   - this has to do with the KL divergence of the risk: a more concentrated \(Q(S|\pi_{i}))\) will be further from the vector of preferences \(C\) than a less concentrated \(Q(S|\pi_{j})\), so the risk ends up being higher for the sensorimotor path the agent is more certain about, again regardless of the final/desired goal state

   - as to why the agent does not switch policy, it has to do with the free energy value of the policy not taken: despite the lower risk, hence lower expected free energy, its free energy is much higher than that of the pursued policy, preventing a re-weighting of the respective policy probabilities

   - it is also worth noting that, despite appearing correct, as things stand the novelty bonuses are basically useless, they are not sufficient for triggering the policy switch

6. lastly, there is the issue of why the \(Q(S|\pi))\) end up being the same for both policies at the end of the experiment:

   - I discovered that /during/ the experiment they are correct, reflecting total uncertainty for the policy not being pursued

   - however, the last update changes things by assigning the wrong probabilities to the policy not being pursued, it is not clear why this is happening, if it is either a bug or an influence of the fact that the agent has collected a full trajectory of observations, a bit more investigation should go into this

** [2024-03-28]: Update on the issue of mistaken \(Q(S|\pi))\)

Point (6) of previous log is incorrect:

- it is not the case that during the experiment the \(Q(S|\pi))\) for the policy not being pursued are correct
- I was looking at the last state, i.e., \(Q(S_{T}|\pi))\), and noticed that during the experiment the probability reflected total uncertainty but peaked incorrectly at the last step
- this happens because effectively at that step the agent receives that observation
- I think the same happens for all the other states

- so what happens is much more nuanced (I'm pretty sure now, 99% of confidence):

  - for the policy being pursued the \(Q(S|\pi))\) are correct throughout the experiment
  - for the other policy the \(Q(S_{i}|\pi))\) are uncertain up to the point the agent receives the observation for state \(i\)

** [2024-03-29]: Update on explosion of B-novelty for pursued policy

After a full day of investigation, I think I have resolved the last apparent incongruity in the simulation with \(\mathbf{B}\)-matrix learning. The strange result was that for the policy being pursued at some point in the experiment (at about episode 141) the \(\mathbf{B}\)-novelty would explode. I inspected extensively info being printed in the terminal related to the computation of the novelty term and realized that, indeed, there was an explosion of the wrong values.

I had to go back to [[cite:&DaCosta2020]] and double-check the analytical expression for the computation of the novelty term. Unfortunately, one has to derive it from that for the \(\mathbf{A}\) matrix because the paper does not do that, and to the best of my knowledge no other active inference paper consider it.

I used the same expression but after some pondering I realized that, while there might be a justification for the way the KL divergence is decomposed (but it is far from obvious and I could provide my personal contribution here), the expression for novelty needs to consider the distribution over the /next/ state and not the current one.

This should be corrected in the paper as well. I wrote and use:

\begin{equation}
\mathbb{E}_{P(O_{\tau}|S_{\tau}) Q(S_{\tau}|\pi_{i})} \Bigl[\mathcal{D}_{KL} \bigl[Q(\mathbf{B}|o_{\tau}, s_{\tau}^{j})|Q(\mathbf{B})\bigr]\Bigr]
\end{equation}

But it should be:

\begin{equation}
\mathbb{E}_{P(O_{\tau+1}|S_{\tau+1}) Q(S_{\tau+1}|\pi_{i})} \Bigl[\mathcal{D}_{KL} \bigl[Q(\mathbf{B}|o_{\tau+1}, s_{\tau+1}^{j})|Q(\mathbf{B})\bigr]\Bigr]
\end{equation}

** [2024-03-31]: Another update on explosion of B-novelty and related incongruities

After fixing the B-novelty term, the worry was that all the behaviour related to policy fixation in the suboptimal agents would disappear. To my surprise, I discovered that the results were confirmed, so my lengthy explanation was not in vain.

However, I did not double check the results with the optimal agents. Today, I discovered that the B-novelty sky-rockets again for these agents after around 140 episodes, but not as much before. I've ran more simulations for more episodes (250 first) and I've discovered that the suboptimal agents disappear, all agents can find the optimal policy at some point. This happens after the B-novelty for policy 1 takes off, pushing the agent to go down the other path in the maze. Now, the risk term seems also to be OK, indicating that my explanation does not apply any more, possibly.

To make sure that the B-novelty does not explode as before I need to run the agents for longer, e.g., 500 episodes. After fixing issues related to divide-by-zero errors in logs (this happens because of the concentration of probabilities).

** [2024-04-05]: Revelations after the Presentation

Did the presentation in front of the people from Chris' lab. The outcome was rather depressing, considering that:

1. I learned that my implementation of active inference is likely a special case of a more general and better approach
2. =pymdp= is a great package written in excellent Python code
3. the results I presented are possibly uninteresting and one of the things I showed is possibly just a quirk of a bad implementation or configuration of the parameters

The only upside is that by looking at =pymdp= and the companion paper I have understood active inference better and how misleading certain theoretical presentation can be (i.e., [[cite:&DaCosta2020]]). In particular, my interpretation and implementation of the framework considered a pure episodic setting in which the agent is allowed to interact with an environment for a fixed number of steps. Instead, in =pymdp= there is no such restriction.

This difference has important ramifications for what it meas to do inference and planning in the respective environments:

1) in my case there is an increasing past horizon and a decreasing future one:

   - free energy is minimized by considering the past, present, and future beliefs for each given policy
   - these beliefs are stored and revised episode after episode
   - planning involves looking into the future based on the knowledge that some time steps have already passed so the future sequence of action is going to be a subset of each policy

2) in their case there is a constant past and future horizon (if we consider the marginal message passing algorithm):

   - this means that at every time step we could think the agent thinks of itself as being at the start state of a POMDP
   - it plans ahead for the full length of a policy regardless of how many time steps have passed during an episode
   - and it looks back for the same number of steps to update its state beliefs conditioned on that policy
   - however those beliefs are not stored and it is not clear what their purpose is in a situation in which there is no learning over \(\mathbf{A}\)-matrices, but I guess the message passing could still be helping with \(\mathbf{B}\)-matrices
   - this scenario is one in which the agent can interact with the environment for how many time steps it is required to reach the goal state, with the expectation that over time the number of time steps will decrease (e.g., DyNA)

This difference was also the source of quite a bit of confusion and misunderstanding during the talk.

While these new discoveries/realizations are certainly welcome insofar as they clarify once more what active inference is and how could be implement, I am left between a rock and a hard place:

- how am I gonna complete the active inference draft now? A clear comparison with =pymdp= appears essential now, explaining at the very least the differences between the two appraoches; of course this requires learning how to use their code
- another pressing point is whether it would make sense to implement their version of active inference in my code, which seems to require a substantial effort/extra work
- also: I'm left wondering about the theoretical justification of /their/ approach since the paper by [[cite:&DaCosta2020]] I think it is better interpreted by my implementation, perhaps there is a connection with control as inference here that should be looked into (/cf/. notes from Levine's course) as well as one with the average reward setting of RL (see [[cite:&Sutton2018]])
  
