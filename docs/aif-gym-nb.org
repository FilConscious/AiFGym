:PROPERTIES:
:CATEGORY: notebook
:ID:       3bb5f4d8-a88a-4d41-8001-62982ba06f78
:END:
#+STARTUP: overview indent

* General Info
* Logs

** 2022-09-14: First simulations

Observations:

- after 50 episodes the agent is not able to learn the correct state-observation mapping
- the only state for which there seems to be some learning is state 5, i.e., \(P(O=5|S=5) = 0.30\)ish (which is very low anyway)
- I think that has some bad consequences for all the other measures/visualization, among others:

  1. the final categorical distributions conditioned on each policy, \(Q(S_{i}|\pi)\), are not correct, for example \(Q(S_{3}|\pi_{3})\)---which is the categorical distribution of the optimal policy (\(\pi_{3}\)) for the state at the third step---puts higher probability mass on state 7 when it should be on state 5;

  2. despite that, the agent consistently picks the optimal policy after 4 episodes, in fact if you look at the other \(Q(S_{i}|\pi_{3})\) they are more or less right so those categorical distributions can be used during planning to select the actions of the optimal policy and the desired trajectory;

  3. point (2) might be indication that perceptual inference is working correctly whereas the Dirichlet update of matrix \(\mathbf{A}\) is not giving the expected results, either because:

     - there is a bug in the code, or
     - when all data from all policies is collated you get nasty updates (consider that when you update the matrix \(\mathbf{A}\) you are using the distribution over states averaged over all policies, but if you have mainly followed one policy those other distribution are likely to be wrong and affect the update badly)

- for some reason all the policies probabilities go to zero at the end of the episode after the fourth episode, not sure if this is a bug
- even so, the optimal policy and the close-to-optimal policy seem to compete for probability mass after the fourth episde (but I don't understand why the both drop to zero at the last step)
- also, the total free energy is zero, this might be another bug

** 2022-09-15: Fixing some issues

Observations:

- after plotting the expected free energy throughout the experiment, I can confirm a couple things:

  1. the expected free energy computation (G) seems to be correct, the optimal policy indeed has a lower EFE than the other one

  2. at the last time step of every episode both policy EFEs go to zero, that means that by looking only at that quantity and updating Q(pi) the two policies will be equiprobable

  3. but (2) is simply the result of a design choice since at the last time step there is no longer any planning to do so the EFE for each policy was set to zero

- even so, (3) should not imply that \(Q(\pi_{1}) = Q(\pi_{2}) = 0.5\) at the end of the episode because in updating the policies probabilities we are also considering the extent to which each policy is able to minimize free energy, i.e., \(Q(\pi_{1}) = \sigma(-\mathcal{F} - \mathcal{G})\) where \(\mathcal{F}\) is the current free energy for \(\pi_{1}\) (it could be described as the evidence for the policy coming from observed outcomes)

- so I now suspect that there might be a problem with \(\mathcal{F}\) that might explain the reason why all the policies probabilities go to zero (observation from previous day)

- indeed, I now notice that the \(\mathcal{F}\) for each policy increases up to a certain point and eventually goes to zero at the end of every episode, I doubt this is right: the "wrong" policy should not have any evidence for it if the optimal policy was taken so it should have high free energy at the end of the episode, conversely the free energy for the optimal policy should decrease as the episode unfolds

- Okay, I fixed the previous issue. The problem was that I messed up the step counts for the training loops, so basically the last agent steps was not carried out, one certain consequence of the bug was that the last free energy slot in the storing matrix/vector stayed at the initialized value of zero (!), it may also be that since the last agent.step was not carried out some parameters were not update, specifically the matrices \(\mathbf{A}\) and \(\mathbf{B}\) (!)

** 2022-09-16: Importance of runs' average

Ran an experiment for 20 episodes to see if there was any improvement. Here are some observations:

- the free energis seem right now, increasing for the wrong policy and decreasing for the optimal one

- despite that, note that for both policies there is an increase in free energy as the episode unfolds, this can be explained by the presense of not-so-perfect state-observation mapping (matrix \(\mathbf{A}\), see observation below), so while you accumulate evidence (observations) for which you are not sure about the free energy might increase

- indeed, 20 episodes (and at the current learning rate, which I think it is just 1) do not seem enough for learning the full state-observation mapping

- there is some learning for the first two states (0 and 1) but at third state already (state 2) the agent is still confused and thinks that \(P(O=2|S=2) = 0.5\)ish, \(P(O=2|S=4) = 0.4\)ish, \(P(O=2|S=0) = 0.3\)ish, of course that affects inference because the agent might infer of being in state 4 after observing 2

- regardless, the agent is again able to pick the optimal policy consistently after a few episodes, having the right transition probability is likely to make this possible/easier even if the state-observation mapping is not perfect

- but likely because of the wrong state observation mapping it does not believe that by following such a policy will lead it to the goal state, i.e., \(Q(S_{8} = 8|\pi_1) = 0.0789\)ish, this appears really strange but it may be due (again) to the misguided state-observation mapping

- what is really strange is that there is somme oscillation at the beginning when it comes to \(Q(S_{8} = 8|\pi_{1})\) with peaks of \(0.8\) but it drops drastically after a few episodes

Ran an experiment for longer, 100 episodes with two different perceptual inference schemes:

1. Gradient descent (the same used until now)

- the drop in \(Q(S_{8} = 8|\pi_{1})\) observed earlier actually happens cyclically, it seems there is a kind of oscillation whereby the probability recovers and then drop again after a while, this might be indication that something is wrong with perceptual inference

- running for more episodes does not seem to lead to better state-observation mappings (matrix A)

2. Setting gradient to zero (analytical solution)

- here something weird happens: the optimal policy all of a sudden produces high free energy so it becomes the "worse" policy to the advantage of the suboptimal policy

- the agent ends up following the suboptimal policy (consistently with the fact that probability over policies is determined by \(\mathcal{F}\) and \(\mathcal{G}\))

- by following the suboptimal policy the agent never gets to the goal state but it is nevertheless compelled to follow it because of the smaller \(\mathcal{F}\)

- if all of that was not weird enough, this time the \(Q(S_{i}|\pi_{0})\) are near perfect even if the state-observation mapping (matrix \(\mathbf{A}\)) is not that comforting

*Hypothesis 1*: there is a problem with perceptual inference, it might be that doing the state updated simultaneuously is not beneficial and the correct variational update should be preferred.

*Hypothesis 2*: there is a problem with action selection at the beginning. Why is the action of the optimal policy not picked at the beginning? Even if its probability is higher? Action selection should be deterministic in this scenario \dots

Ran an experiment with NO learning over matrices \(\mathbf{A}\) and \(\mathbf{B}\):

- here everything seems to work fine

- however note the interaction in the update of \(Q(S|\pi_{0})\) with the wrong evidence

Ran another experiment for just a bunch of episodes with learning of matrix \(\mathbf{A}\) to see what happens at the beginning of the experiment:

- again, a scrambled state-observation mapping notwithstanding, the expected free energy makes action selection opting for the optimal policy

- however, the free energy value for the optimal policy turns out to be alarming, depsite the fact that the agent is collecting evidence for the optimal policy its free energy increases, the opposite happens for the suboptimal policy, this is really puzzling

- of course, if you were to include the \(\mathcal{F}\) in the action selection procedure, as things stand the worse policy would be selected instead

- another puzzling things is that the \(Q(S|\pi_{0})\) are near perfect even if that policy is rarely picked!

Explanation of what might be happening (come up after running): at the beginning of the experiment the agent might start going down the right path by picking the action from the optimal policy, note that at this stage the free energies associated with both policy might be very similar, in the middle of the path it may also happen that the free energy for the suboptimal policy turns out to be slightly smaller (why this happens is still a mistery) bringing the agent to select an action from the suboptimal policy, in a perverse turns of events (somehow) this leads to an even smaller free energy for the suboptimal policy, a pernicious cascade ensues so that at the next episode the agent keep selecting actions from the suboptimal policy; unfortunately expected free energy does not help either because the free energy becomes too big quickly (for the optimal policy thereby penalizing it) or because state-observation mapping remain all scrambled. Now, a bunch of hypothesis:

- it could be that the analytic implementation is not correct (note: these issues are not present in the gradient implementation)
- it could be this is just a quirk for this run and training for more than one agent might reveal a different picture (indeed, this might be an example of a bad bootstrap)
- Indeed, it was a quirk about the run, I forgot that for these kinds of randomness-imbued experiments it is crucial to average over runs/agents!

# Test 1.1: Learning State Observation Mapping (matrix A) with Knowledge of State-Transitions (matrix B)
# Test 2: Learning State Observation Mapping (matrix A) with Knowledge of State-Transitions (matrix B)

** 2023-07-03: Updated status

This was put on hold for several months (due to writing for the dissertation). The code is there but it may need some polishing before making the repo public and proper documentation. Also, the idea was to integrate the Monte-Carlo tree search planning component for a basic comparison with a RL agent.

** 2023-07-11: Added docs folder

I added a docs folder storing a file for the documentation, that can be accessed from the README.org, and this notebook for observations, notes, etc. (similar to what done for the predictive coding repo). Next, I should proceed to fill in those file with useful and essential information.

** 2023-07-17: Writing the Repo Overview

I'm going through the repo to summarize how it works and eliminate redundant, old files.

List of files that can be deleted:

- ~../scripts/utils_run.py~ can be deleted as it belongs to a previous implementation
-

** 2023-07-27: Issue about policy-independent state probabilities

I found some duplicate code that computes policy-independent state probabilities, reflecting a doubt that I had about /when/ to perform that computation, after state-estimation (perception) or after policy updating (planning)?

I think it makes more sense to do it after policy updating (and that is probably why I had commented out the first instance of those lines of code). However, those updated probabilities are usually not used by the agent, except maybe in the implementation of [[cite:&Sales2019]] (I need to double-check this).

** 2023-10-04: Pip issue with setuptools installation

I stumbled upon the following error when trying to install the AiFGym package with ~pip install -e .~:

~ERROR: Could not find a version that satisfies the requirement python==3.10 (from versions: none)~
~ERROR: No matching distribution found for python==3.10~

The error seems to be about ~pip~ not finding the required version of Python (as specified in the =project.toml= file of the package).

But I am running the installation inside a new conda environment created with an appropriate version of Python. So I don't understand why ~pip~ is throwing this error.

After removing two lines from the =project.toml= that specified Python as a requirement, the installation worked and all the dependencies were installed.

It seems the installation procedure failed to recognize the version of Python installed in the activated conda environment.

** 2023-10-05: Pip issue resolved

The mistake/bug was listing python among the dependencies in the =pyproject.tom=, having =requires-python = ">=3.10"= under =[project]= heading is sufficient.

** 2023-10-30: Errors with relative import

Upon trying to run a second experiment with active inference, I've got a relative import error. This might be due to some modification I made to configure the setuptools functionality for the installation of the repo.

The line throwing the error was in the =main.py=, i.e.:

~task_module = importlib.import_module("..active_inf.tasks." + task_module_name)~

And the complaint was ~ImportError: attempted relative import beyond top-level package~. This happens when you run the main script with ~python -m scripts.main -task task1 -env GridWorldv0 -nr 1 -ne 2 -pt states -as kd~ /inside/ the cloned repo directory.

I think that using that instruction tells Python to consider =script= as the top-level package (instead of =AiFGym=). When you want to import the task module with the =importlib= functionality, using the two dots asks Python to go beyong that top-level package, which is not allowed, apparently; rather, Python does not have or store information about what there is beyond that point (see these *stack overflow* answers [[https://stackoverflow.com/questions/30669474/beyond-top-level-package-error-in-relative-import][beyond top level package error in relative import]] and other links therein).

Before I added the setuptools functionality the code was working fine. I suspect that adding that functionality or adding some =ini.py= or changing the package structure was responsible for the error, but I cannot pinpoint exactly what produced it.

Thinking more about it, it might have to do with the fact that this time I cloned and /installed/ the package with ~pip install --editable .~ this time. According to the explanation at [[https://www.reddit.com/r/learnpython/comments/ayx7za/how_does_pip_install_e_work_is_there_a_specific/?rdt=36064][How does `pip install -e` work?]], that instruction installs (copies) the package in the =site-package= folder of your Python installation. Now the question is whether running the main Python module with  ~python -m AiFGym.scripts.main ...~ is actually using the site-package version of the package.

Mmmh... Maybe not because when I used ~python -m scripts.main -task ...~ there was no package indication. However, that might still be important because, as it is clear in the solution below, I have to specify the package name now, i.e., =AiFGym= in the ~importlib~ command. Python knows about that package becaue I have installed it.

The implemented solution involves two main corrections:

1. To specify that ~active_inf~ is a submodule of the ~AiFGym~ package, by writing:

   ~task_module = importlib.import_module(".active_inf.tasks." + task_module_name, "AiFGym")~

2. Run/invoke the Python module ~script.main~ from the directory in which the package ~AiFGym~ is located and by adding the specification of that package, i.e., ~AiFGym.script.main~. This is essential otherwise Python complains that it cannot find any ~AiFGym~ module after you make correction (1).

One of the downside of this solution is that the downloaded package (i.e., the cloned repo) cannot be renamed.

** 2023-10-31: ModuleNotFound Error

The blocking issue today was that running ~python -m AiFGym...~ threw a =ModuleNotFound= error. That was unexpected because installing the package with ~pip install -e .~ should make the package visible to Python (in the =site-package= of the Python installation of the created conda environment).

After browsing for more knowledge on how Python looks for modules locally, it turns out that the basic error I made was having all the code inside a directory whose name (=/./cde-AifGym=) did not correspond to the name of the installed package (=AiFGym=). That directory was stored in the =site-package= folder of the package, but this tells Python to look into /that/ folder for a package name =AiFGym=, which was not there.

The solution is simply to create another folder called =AiFGym= inside =/./cde-AiFGym/=. Actually it is not that simple because I'm using a flat layout so I have to run ~pip install -e .~ inside the =/AiFGym= folder which then makes Python look inside that folder for the package ~AiFGym~, but the package is not inside itself!

After some reflections, it seems all was due to a bad repo structure. To do successfully all the things above, you need to have a /project root directory/ which contains the =pyproject.toml=, the license, the =README.md= etc. as well as the main package you want to install/use, i.e., ~aifgym~. All the relevant code should go in there so it was a mistake to take out the =env= and the =scripts= folders in this case.

Now I have to update all the dependencies and pointers to folders inside the various files...then try again the installation and running some experiments. Let's see if I got it right this time!
